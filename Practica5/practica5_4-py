import numpy as np
import matplotlib.pyplot as plt
from scipy.io import loadmat
import scipy.optimize as opt

def hipotesis(theta,X):
    return X.dot(theta)

def coste(thetas, matrizX, vectorY, λ=0.):
    nMuestras = matrizX.shape[0]
    hipo = hipotesis(thetas,matrizX).reshape((nMuestras,1))
    cost = float((1./(2*nMuestras)) * np.dot((hipo-vectorY).T,(hipo-vectorY))) + (float(λ)/(2*nMuestras)) * float(thetas[1:].T.dot(thetas[1:]))
    return cost 

def gradiente(thetas, matrizX, vectorY, λ=0.):
    thetas = thetas.reshape((thetas.shape[0],1))
    nMuestras = matrizX.shape[0]    
    grad = (1./float(nMuestras))*matrizX.T.dot(hipotesis(thetas,matrizX)-vectorY) + (float(λ)/nMuestras)*thetas
    return grad

def gradiente_min(thetas, matrizX, vectorY, λ=0.):
    return gradiente(thetas, matrizX, vectorY, λ=0.).flatten()

def optimizarTheta(thetas, matrizX, vectorY, λ=0.,_print=True):
    return opt.fmin_cg(coste,x0=thetas,fprime=gradiente_min, args=(matrizX,vectorY,λ),disp=_print, epsilon=1.49e-12, maxiter=1000)


# Recibe una matriz X (dimensiones m*l) y devuelve otra con dimensiones m*p
def generar_dimension(matrizX, p):    
    for i in range(p):
        dim = i+2
        matrizX = np.insert(matrizX,matrizX.shape[1],np.power(matrizX[:,1],dim),axis=1)
    return matrizX
#Funcion para evitar grandes diferencias de rango
def normalizar_atributos(matrizX):    
    medias = np.mean(matrizX,axis=0) 
    matrizX[:,1:] = matrizX[:,1:] - medias[1:]
    desviaciones = np.std(matrizX,axis=0,ddof=1)
    matrizX[:,1:] = matrizX[:,1:] / desviaciones[1:]
    return matrizX, medias, desviaciones #MatrizX esta normalizada

def tryλ_values(maxLambda,numSteps):
    Theta = np.zeros((X_poli_norm.shape[1],1))
    lambdas = np.linspace(0,maxLambda,numSteps)
    vector_train, vector_val = [], []
    for lamb in lambdas:
        train_aux = generar_dimension(X_unos, grado_polinomio)
        train_aux_norm, aux1, aux2 = normalizar_atributos(train_aux)
        val_aux = generar_dimension(Xval, grado_polinomio)
        val_aux_norm, aux1, aux2 = normalizar_atributos(val_aux)
        ini_theta = np.ones((X_poli_norm.shape[1],1))
        theta_opt = optimizarTheta(Theta, train_aux_norm, y, lamb, False)
        vector_train.append(coste(theta_opt, train_aux_norm, y, λ=lamb))
        vector_val.append(coste(theta_opt, val_aux_norm, yval, λ=lamb))

    plt.figure()
    title='Selecting λ using a cross validation set'
    plt.title(title)
    plt.plot(lambdas, vector_train, label='Train')
    plt.plot(lambdas, vector_val, label='Cross Validation')
    plt.legend()
    plt.xlabel('lambda')
    plt.ylabel('Error')
    plt.savefig(title+'.png')
    plt.show()


data = loadmat('ex5data1.mat')
y = data['y']
X = data['X']

yval = data['yval']
Xval = data['Xval']

X_unos = np.insert(X,0,1,axis=1)
Xval = np.insert(Xval,0,1,axis=1)

Theta = np.ones((2,1))
theta_opt = optimizarTheta(Theta,X_unos,y,0.)
grado_polinomio = 8
X_poli = generar_dimension(X_unos,grado_polinomio)
X_poli_norm, media, desviacion = normalizar_atributos(X_poli)
tryλ_values(5,100)
